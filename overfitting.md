# Overview
Overfitting refers to trained machine learning models that are too flexible, leading to poor out-of-sample prediction. That is, the in-sample prediction errors are much smaller than the out-of-sample prediction errors.

Understanding this more deeply, requires a theory of data variation. Data variation is typically categorised as being components of:

1. Systematic variation
2. Idiosyncratic variation

Systematic variation can be thought of as either linear or non-linear functional relationship between two or more variables. Mispecifying the functional relationship can lead to systematic error.

Idiosyncratic variation suggests there is a source of variation that cannot be controlled via a control variable. There are various types of idiosyncratic variation:
1. Random error caused by natural and chaotic disturbances
2. Measurement error caused by an imprecise or faulty sensor

# Remedies 
Overfitting can be controlled or reduced by:
1. Adding more examples to a training data set
2. Reducing the number of [[features]] in a machine learning model
3. [[regularisation]]




 