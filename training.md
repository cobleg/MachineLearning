# Overview
Once the code and input data are prepared for a machine learning model, a new model can be trained.

The objective of training a model is to create a prediction rule that generalises well to new data not yet observed. 

# Training steps
Execute the following loop:

-   Initialize parametersÂ $(ğ°,ğ‘)$ 
-   Repeat until done
    -   Compute gradientÂ $ğ â†âˆ‚(ğ°,ğ‘)\frac{1}{|B|}\Sigma_{ğ‘–âˆˆ}l(ğ±^{ğ‘–},ğ‘¦^{ğ‘–},ğ°,ğ‘)$ 
    -   Update parametersÂ $(ğ°,ğ‘)â†(ğ°,ğ‘)âˆ’ğœ‚ğ $

# Issues in training models
There are some common problems that occur during model training, such as overfitting and underfitting the received data. This can occur when the process that generates the data (e.g. a sensor attached to a machine) produces data with uncontrolled variation. One source of uncontrolled variation can be randomness often called noise. This could be simple measurement error. There can also be systematic uncontrolled variation caused by some other process variable that is not included in the training data set.

There are algorithms available (e.g. the Kalman filter) that remove a lot of noise but cannot adjust for a missing input variable. So it is important to achieve the best fitting model that can produce predictions over unobserved data that are within a prescribed tolerance level.

This acknowledges that there are reasons why the best predictive model is not necessarily the trained model that produces the least amount of error (i.e. the difference between the observed and predicted values) across a training data set.

## Overfitting
The model is [overfitted](https://www.ibm.com/cloud/learn/overfitting) when it fails to distinguish between the signal and the uncontrolled variation embedded in the training data. This can happen when the model being trained is too flexible.

### Causes of overfitting
- Too little data
- Too many features
- 

### Solutions to overfitting
There are several methods that can reduce overfitting during training such as:
- Collect more data, i.e. more input and target examples
- Reducing the number of features in a training set. In particular polynomials of high degree. 
- Regularisation, which reduces the magnitude of some of the estimated weights.

## Underfitting
The model is [underfitted](https://www.ibm.com/cloud/learn/overfitting) when the model fails to recognise the systematic pattern contained in the training data. This can happen when the model being trained is too inflexible. For example, a linear model fitted to data that is generated by a non-linear systematic process.

# Code

```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # Minibatch loss in `X` and `y`
        # Compute gradient on `l` with respect to [`w`, `b`]
        l.sum().backward()
        sgd([w, b], lr, batch_size)  # Update parameters using their gradient
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

`lr` is the learning rate
`epoch` An epoch means training the neural network with all the training data for one cycle. In an epoch, we use all of the data exactly once. A forward pass and a backward pass together are counted as one pass. [Epoch in neural networks](https://www.baeldung.com/cs/epoch-neural-networks)
